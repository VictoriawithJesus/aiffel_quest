# Rossmann Store Sales
목표: 매장, 프로모션, 경쟁사 데이터를 활용하여 매출 예측  
1,115개 Rossmann 매장에 대한 과거 판매 데이터가 제공, 작업은 테스트 세트의 "판매" 열을 예측하는 것

- file  
train.csv : 매출을 포함한 과거 데이터  
test.csv : 매출을 제외한 과거 데이터  
Sample_submission.csv : 올바른 형식의 샘플 제출 파일  
store.csv : 매장에 대한 추가 정보

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
## 기술통계 및 추정
### 데이터셋의 전체 구조 파악
#### train.csv
##### 데이터 불러오기
train = pd.read_csv('/Users/project/aiffel/rossmann/train.csv')
train.head()
##### 기본 정보 확인
Store: 각 매장의 고유 ID  
DayOfWeek: 주중을 나타내는 숫자 (1에서 7까지, 1이 일요일)  
Date: 날짜 정보  
Sales: 특정 날짜의 매출(예측하는 내용)  
Customers: 특정 날짜의 고객 수  
Open: 상점이 열려있는지 여부  
Promo: 프로모션 여부   
StateHoliday: 공휴일 (a = 공휴일, b = 부활절 휴일, c = 크리스마스, 0 = 없음)  
- 일반적으로 공휴일에는 몇몇 매장을 제외하고 모든 매장이 문을 닫는다.
- 공휴일과 주말에는 모든 학교가 문을 닫는다.  

SchoolHoliday: (상점, 날짜)가 공립학교 폐쇄로 인해 영향을 받았는지 여부  
train.info()
# object type 이라서 datetime 으로 바꿔준다.
train['Date'] = pd.to_datetime(train['Date'])
##### 기술통계량 확인
train.describe()
##### 유니크 값 확인
train.nunique()
#### test.csv
##### 데이터 불러오기
test = pd.read_csv('/Users/project/aiffel/rossmann/test.csv')
test.head()
##### 기본 정보 확인
Id: 데이터 엔트리 식별
Store: 상점 식별자  
DayOfWeek: 주중을 나타내는 숫자 (1에서 7까지, 1이 일요일)  
Date: 날짜 정보   
Open: 상점이 열려있는지 여부  
Promo: 프로모션 여부   
StateHoliday: 공휴일 (a = 공휴일, b = 부활절 휴일, c = 크리스마스, 0 = 없음)  
- 일반적으로 공휴일에는 몇몇 매장을 제외하고 모든 매장이 문을 닫는다.
- 공휴일과 주말에는 모든 학교가 문을 닫는다.  

SchoolHoliday: (상점, 날짜)가 공립학교 폐쇄로 인해 영향을 받았는지 여부  
test.info()
# object type 이라서 datetime 으로 바꿔준다.
test['Date'] = pd.to_datetime(test['Date'])
##### 기술통계량 확인
test.describe()
##### 유니크 값 확인
test.nunique()
#### store.csv
##### 데이터 불러오기
store = pd.read_csv('/Users/project/aiffel/rossmann/store.csv')
store.head()
##### 기본 정보 확인  
Store: 상점 식별자  
StoreType: 상점 유형 ('a', 'b', 'c', 'd')  
Assortment: 구색 수준 (a = 기본, b = 추가, c = 확장)  
CompetitionDistance: 가장 가까운 경쟁사 매장까지의 거리(미터)  
CompetitionOpenSinceMonth: 가장 가까운 경쟁자가 개설된 대략적인 월  
CompetitionOpenSinceYear: 가장 가까운 경쟁자가 개설된 대략적인 년  
Promo2: 일부 매장에 대한 지속적이고 연속적인 프로모션입 (0 = 매장이 참여하지 않음, 1 = 매장이 참여 중)  
Promo2SinceWeek: 매장이 Promo2에 참여하기 시작한 주  
Promo2SinceYear: 매장이 Promo2에 참여하기 시작한 년  
PromoInterval: Promo2가 시작되는 연속 간격을 설명하고 프로모션이 새로 시작되는 달의 이름을 지정
- 예: "2월,5월,8월,11월"은 각 라운드가 해당 매장의 특정 연도 2월, 5월, 8월, 11월에 시작됨을 의미
store.info()
##### 기술통계량 확인
store.describe()
##### 유니크 값 확인
store.nunique()
### 결측치 탐색 및 처리
#### train
##### 결측치 탐색
train.isna().sum()
##### 각 칼럼 유니크 값 확인
train['Store'].unique()
train['Open'].unique()
train['Promo'].unique()
# a = 공휴일, b = 부활절 휴일, c = 크리스마스, 0 = 없음
print('State Holiday unique : ', train['StateHoliday'].unique())
print('State Holiday unique dtype : ', train['StateHoliday'].unique().dtype)
print('State Holiday nunique : ', train['StateHoliday'].nunique())
train[train['StateHoliday'] == 0]
train[train['StateHoliday'] == '0']
# 데이터 일관성을 위해 0을 문자열로 바꿔준다.
train['StateHoliday'] = train['StateHoliday'].replace(0, "0")
train['StateHoliday'].unique()
train['SchoolHoliday'].unique()
##### 결측치 처리

결측치가 없으므로 처리하지 않는다.
#### test
##### 결측치 탐색
test.info()
test.isna().sum()
test['StateHoliday'].unique().dtype
##### 결측치 있는 칼럼 확인
test[test['Open'].isna()]
test['Open'].unique()
##### 결측치 처리
- 선형 보간 (Linear Interpolation)  
결측값을 주변 값들의 선형 보간으로 채우는 방법
linear = test.copy()

linear['Open'].interpolate(method='linear', inplace=True)
test.loc[test['Open'].isna()]
linear.loc[test['Open'].isna()]
- 다항 보간(polynomial interpolate)  
결측치를 다항식을 사용하여 보간하는 방법 
polynomial = test.copy()

polynomial['Open'].interpolate(method='polynomial', order=2, inplace=True)
test.loc[test['Open'].isna()]
polynomial.loc[test['Open'].isna()]
- 스플라인 보간(Spline interpolate)  
결측치를 다항식 대신 조각별로 정의된 부드러운 곡선 또는 다항식(스플라인)을 사용하여 보간하는 방법
spline = test.copy()

spline['Open'].interpolate(method='spline', order=5, inplace=True)
test.loc[test['Open'].isna()]
spline.loc[test['Open'].isna()]
- 콜드덱(Cold-deck)  
이전 값과 다음 값의 평균을 사용하여 결측치를 채우는 것  
시계열 데이터에서는 이전 시점의 값이 다음 시점에 영향을 줄 수 있기 때문에 사용
cold = test.copy()

cold['Open'].interpolate(method='pad', inplace=True)
test.loc[test['Open'].isna()]
cold.loc[test['Open'].isna()]
- 핫덱(Hot-deck)  
결측치를 다른 유사한 관측치의 값으로 채우는 방법  
결측치를 가진 관측치와 유사한 다른 관측치를 찾아서 그 값으로 결측치를 채우는 방식
hot = test.copy()

hot['Open'].interpolate(method='nearest', inplace=True)
test.loc[test['Open'].isna()]
hot.loc[test['Open'].isna()]
- 결측치 처리  
여러가지 방법을 사용해본 결과, 전부 1.0으로 채워지기 때문에 스플라인 보간법을 제외한 방법으로 결측치를 처리할 것이다.  
그리고, open dtype이 float이기 때문에 int로 바꾸어 데이터 일관성을 유지해줄 것이다.
test.loc[test['Open'].isna()]
test['Open'].interpolate(method='linear', inplace=True)
test[test['Open'].isna()]
test['Open'] = test['Open'].astype(int)
test['Open'].dtype
#### store
##### 결측치 탐색
store.info()
store.isna().sum()
##### 결측치 있는 칼럼 확인
store[store['CompetitionDistance'].isna()]
store[store['CompetitionOpenSinceMonth'].isna()]
store[store['CompetitionOpenSinceYear'].isna()]
store[store['CompetitionOpenSinceYear'].isna() & store['CompetitionOpenSinceMonth'].isna()]
store[store['Promo2SinceWeek'].isna()]
store[store['Promo2SinceYear'].isna()]
store[store['PromoInterval'].isna()]
store[store['PromoInterval'].isna() & store['Promo2SinceWeek'].isna() & store['Promo2SinceYear'].isna()]
##### 결측치 처리
store는 매장의 추가 정보가 들어 있는 데이터 이기 때문에 따로 결측치 처리를 해주지 않는다.
### 이상치 탐색 및 처리
#### train
##### 이상치 탐색
# 숫자 컬럼만 선택
numerical_cols = train.select_dtypes(include=[np.number]).columns.tolist()

# 서브블롯의 행을 계산
num_cols = len(numerical_cols)
num_rows = num_cols // 4
num_rows += num_cols % 4

position = range(1, num_cols + 1)

fig = plt.figure(figsize=(16, num_rows * 4))

#서브플롯으로 boxplot 그리기
for k, col in zip(position, numerical_cols):
  ax = fig.add_subplot(num_rows, 4, k)
  sns.boxplot(train[col], ax=ax)
  ax.set_title(col)

plt.tight_layout()
plt.show()
##### 이상치 처리

Sales, Customer 에 대하여 이상치가 있지만 
#### test
##### 이상치 탐색
# 숫자 컬럼만 선택
numerical_cols = test.select_dtypes(include=[np.number]).columns.tolist()

# 서브블롯의 행을 계산
num_cols = len(numerical_cols)
num_rows = num_cols // 4
num_rows += num_cols % 4

position = range(1, num_cols + 1)

fig = plt.figure(figsize=(16, num_rows * 4))

#서브플롯으로 boxplot 그리기
for k, col in zip(position, numerical_cols):
  ax = fig.add_subplot(num_rows, 4, k)
  sns.boxplot(test[col], ax=ax)
  ax.set_title(col)

plt.tight_layout()
plt.show()
##### 이상치 처리

이상치가 없으므로 처리하지 않는다.
#### store
##### 이상치 탐색
# 숫자 컬럼만 선택
numerical_cols = store.select_dtypes(include=[np.number]).columns.tolist()

# 서브블롯의 행을 계산
num_cols = len(numerical_cols)
num_rows = num_cols // 4
num_rows += num_cols % 4

position = range(1, num_cols + 1)

fig = plt.figure(figsize=(16, num_rows * 4))

#서브플롯으로 boxplot 그리기
for k, col in zip(position, numerical_cols):
  ax = fig.add_subplot(num_rows, 4, k)
  sns.boxplot(store[col], ax=ax)
  ax.set_title(col)

plt.tight_layout()
plt.show()
##### 이상치 처리

CompetitionDistance, CompetitionOpenSinceYear 에 대해 이상치가 있지만
### Feature Engineering
#### train
train.head()
train.info()
##### Date
날짜 데이터에서 연도, 월, 일 등 추출
train['Year'] = train['Date'].dt.year
train['Month'] = train['Date'].dt.month
train['Day'] = train['Date'].dt.day
# 칼럼 재배치
train = train[['Store', 'Date', 'Year', 'Month', 'Day', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']]
train.head()
##### monthly sales
monthly_sales = train.groupby(['Year', 'Month'], as_index=False)[['Sales', 'Customers']].mean()
monthly_sales
train = pd.merge(train, monthly_sales, on=['Year', 'Month'], how='left', suffixes=('', '_avg'))
train.head()
train.info()
#### test
test.head()
test.info()
##### Date
날짜 데이터에서 연도, 월, 일 등 추출
test['Year'] = test['Date'].dt.year
test['Month'] = test['Date'].dt.month
test['Day'] = test['Date'].dt.day
# 칼럼 재배치
test = test[['Id', 'Store', 'Date', 'Year', 'Month', 'Day', 'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']]
test.head()
test.info()
#### store
store.head()
store.info()
train.head()
store.head()

### 상간관계 분석
#### train
train.select_dtypes(include='number').corr()
plt.figure(figsize=(10, 7))
sns.heatmap(train.select_dtypes(include='number').corr(), annot=True)
#### test
test.select_dtypes(include='number').corr()
plt.figure(figsize=(10, 7))
sns.heatmap(test.select_dtypes(include='number').corr(), annot=True)
#### store
store.select_dtypes(include='number').corr()
plt.figure(figsize=(10, 7))
sns.heatmap(store.select_dtypes(include='number').corr(), annot=True)
### 시각화를 통한 데이터 탐색
#### train
train.info()
- 일별 매출과 고객 수 시계열 그래프  
각 날짜에 대한 매출과 고객 수를 시간에 따라 시각화
plt.figure(figsize=(14, 7))
plt.plot(train['Date'], train['Sales'], label='Sales')
plt.plot(train['Date'], train['Customers'], label='Customers')
plt.title('Daily Sales and Customers Over Time')
plt.xlabel('Date')
plt.ylabel('Count')
plt.legend()
plt.show()
- 월별 매출 평균과 고객 수 평균 바 그래프  
각 월에 대한 매출과 고객 수의 평균을 바 그래프로 시각화
monthly_sales = train.groupby(['Year', 'Month'])[['Sales', 'Customers']].mean().reset_index()

plt.figure(figsize=(12, 6))
sns.barplot(x='Month', y='Sales', data=monthly_sales, hue='Year', ci=None)
plt.title('Monthly Average Sales Over the Years')
plt.xlabel('Month')
plt.ylabel('Average Sales')
plt.show()
- 휴일 여부에 따른 매출 비교  
휴일인 날과 아닌 날의 평균 매출을 시각화
holiday_sales = train.groupby('StateHoliday')['Sales'].mean().reset_index()

plt.figure(figsize=(8, 5))
sns.barplot(x='StateHoliday', y='Sales', data=holiday_sales)
plt.title('Average Sales on Holidays vs. Non-Holidays')
plt.xlabel('StateHoliday')
plt.ylabel('Average Sales')
plt.show()
- 매장의 영업 여부에 따른 매출 비교  
매장이 오픈한 날과 그렇지 않은 날의 매출을 비교
open_closed_sales = train.groupby('Open')['Sales'].mean().reset_index()

plt.figure(figsize=(8, 5))
sns.barplot(x='Open', y='Sales', data=open_closed_sales)
plt.title('Average Sales on Open vs. Closed Days')
plt.xlabel('Open')
plt.ylabel('Average Sales')
plt.show()
- 프로모션 여부에 따른 매출 비교  
프로모션을 진행한 날과 그렇지 않은 날의 매출을 비교
promo_sales = train.groupby('Promo')['Sales'].mean().reset_index()

plt.figure(figsize=(8, 5))
sns.barplot(x='Promo', y='Sales', data=promo_sales)
plt.title('Average Sales on Promo vs. Non-Promo Days')
plt.xlabel('Promo')
plt.ylabel('Average Sales')
plt.show()
- 매출 및 고객수 분포  
매출과 고객 수의 분포를 히스토그램으로 시각화
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(train['Sales'], bins=50, kde=True, color='skyblue')
plt.title('Distribution of Sales')

plt.subplot(1, 2, 2)
sns.histplot(train['Customers'], bins=50, kde=True, color='salmon')
plt.title('Distribution of Customers')

plt.tight_layout()
plt.show()
- 월별 매출 추이  
전체 매출 및 고객 수의 월별 추이 시각화
monthly_sales = train.groupby(['Year', 'Month'])[['Sales', 'Customers']].sum()

plt.figure(figsize=(14, 6))
monthly_sales['Sales'].plot(marker='o', label='Sales', color='skyblue')
monthly_sales['Customers'].plot(marker='o', label='Customers', color='salmon')
plt.title('Monthly Sales and Customers Trend')
plt.xlabel('Year-Month')
plt.ylabel('Count')
plt.legend()
plt.show()
- 매출과 고객 수의 산점도  
매출과 고객 수 간의 관계를 산점도
plt.figure(figsize=(10, 6))
plt.scatter(train['Customers'], train['Sales'], alpha=0.3, color='green')
plt.title('Sales vs. Customers')
plt.xlabel('Customers')
plt.ylabel('Sales')
plt.show()
## 추정
### 모집단의 특성 추정
#### train
train.info()
기술 통계량 확인

1. 매출 (Sales) 및 고객 수 (Customers)  
평균적으로 하나의 매장에서 하루 평균 약 5773의 매출이 발생하고, 약 633명의 고객이 방문하는 것으로 나타난다.  
최소 매출은 0이며, 최대 매출은 41551이다.  
고객 수의 경우 최소 0부터 최대 7388까지의 값이 있다.  
두 변수 모두 분포가 오른쪽으로 치우쳐져 있다.  

2. 매장 운영 (Open) 및 프로모션 (Promo)  
평균적으로 매장이 열려있는 날은 전체의 약 83%, 프로모션이 적용된 날은 약 38%로 나타난다.  
매장 운영과 프로모션은 이진 변수이므로 평균값이 해당 변수가 참일 확률을 나타난다.  
  
3. 학교 휴일 (SchoolHoliday)  
평균적으로 학교 휴일이 발생하는 날은 전체의 약 18%로 나타난다.  

4. 날짜 정보 (Year, Month, Day, DayOfWeek)
기록된 날짜 범위는 2013년부터 2015년까지로 나타난다.  
월, 일, 요일에 대한 특성을 나타내는 열도 있다.  

5. 평균 매출 및 평균 고객 수 (Sales_avg, Customers_avg)  
평균 매출과 평균 고객 수는 각각 5773과 633으로 나타난다.
train.describe()
상관 관계  
매출(Sales)과 관련된 변수들 간에 강한 상관 관계가 있음을 확인하였다.
train.select_dtypes(include='number').corr()
시간에 따른 변화 확인  
'Year', 'Month' 등의 시간 관련 피처를 사용하여 'Sales'의 시간적 변화를 확인
monthly_sales = train.groupby(['Year', 'Month'])['Sales'].mean().reset_index()
plt.figure(figsize=(12, 6))
sns.lineplot(x='Month', y='Sales', hue='Year', data=monthly_sales)
plt.title('Monthly Sales Over the Years')
plt.show()
휴일 여부에 따른 변화 확인  
'StateHoliday', 'SchoolHoliday' 등의 피처를 사용하여 휴일에 따른 'Sales'의 변화를 확인
state_holiday_sales = train.groupby('StateHoliday')['Sales'].mean().reset_index()
plt.figure(figsize=(8, 5))
sns.barplot(x='StateHoliday', y='Sales', data=state_holiday_sales)
plt.title('Sales During State Holidays')
plt.show()
다른 요인들에 따른 변화 확인  
'StoreType', 'Assortment', 'Promo' 등의 다른 피처들에 따라 'Sales'의 변화를 확인
merged_data = pd.merge(train, store, on='Store', how='left')
plt.figure(figsize=(14, 6))

plt.subplot(1, 3, 1)
sns.boxplot(x='StoreType', y='Sales', data=merged_data)
plt.title('Sales Across Store Types')

plt.subplot(1, 3, 2)
sns.boxplot(x='Assortment', y='Sales', data=merged_data)
plt.title('Sales Across Assortments')

plt.subplot(1, 3, 3)
sns.boxplot(x='Promo', y='Sales', data=merged_data)
plt.title('Sales During Promotions')

plt.show()
모집단 특성 추정  
평균 매출 (Sales): 5773.818972305593  
표준 편차: 3849.92617523476
population_mean = train['Sales'].mean()
population_std = train['Sales'].std()

print(f"Population Mean: {population_mean}")
print(f"Population Standard Deviation: {population_std}")
### 표본 크기 조정
표본 크기 계산에서 필요한 표본 크기는 1569.7721019023288로 나왔지만   
이미 사용된 표본 크기가 0.80으로 적절하게 조정되었다.
import statsmodels.stats.api as sms

# 효과 크기 설정
effect_size = 0.1

# 표본 크기 계산
nobs1 = sms.NormalIndPower().solve_power(effect_size, alpha=0.05, power=0.8, ratio=1)

# 조정된 표본 크기 계산
nobs_adjusted = sms.NormalIndPower().solve_power(effect_size, alpha=0.05, nobs1=nobs1, ratio=1, alternative='two-sided')

# 결과 출력
print(f"Required Sample Size: {nobs1}")
print(f"Adjusted Sample Size: {nobs_adjusted}")

### 추정 오차 분석

표본의 평균 매출: 5645.465901848311   
표준 편차: 4009.1296563015803  
95%의 신뢰도로 모집단 평균에 대한 신뢰 구간:  (5446.93775674649, 5843.994046950132)
# 표본 추출
sample_data = train['Sales'].sample(n=int(nobs1), random_state=42)

# 추정된 표본 특성 계산
sample_mean = sample_data.mean()
sample_std = sample_data.std()

# 표준 정규 분포에서의 신뢰구간 계산
confidence_interval = sms.DescrStatsW(sample_data).tconfint_mean(alpha=0.05)

# 결과 출력
print(f"Sample Mean: {sample_mean}")
print(f"Sample Standard Deviation: {sample_std}")
print(f"Confidence Interval: {confidence_interval}")
## 검정
### 가설 설정: 귀무가설과 대립가설
귀무가설 (H0)  
매장의 평균 매출(Sales)은 모든 매장에서 동일하다.
대립가설 (H1)  
매장의 평균 매출(Sales)은 적어도 하나의 매장에서 다르다.
### 검정 수행
#### ANOVA (Analysis of Variance)  
모든 매장 간 평균 매출이 동일한지 확인하는 분산 분석(ANOVA)을 수행
train['Store'].unique()
from scipy import stats

# One-way ANOVA
f_statistic, p_value_anova = stats.f_oneway(*[train['Sales'][train['Store'] == store] for store in train['Store'].unique()])

print(f'ANOVA f-statistic : {f_statistic}')
print(f'ANOVA p-value: {p_value_anova}')
f-statistic 값이 크고 p-value가 0.0으로 나왔다.  
이는 각 매장의 평균 매출이 모두 동일하다는 귀무가설을 기각할 충분한 증거가 있다.  
다시 말해, 적어도 하나의 매장은 다른 매장들과 평균 매출이 다르다고 할 수 있다. 

해석:
ANOVA 결과 f-statistic 값이 클수록 그룹 간의 차이가 크다는 의미한다. 여기서는 매장 간 차이가 크다는 것을 나타낸다.  
p-value가 0.05보다 작으므로 귀무가설을 기각한다. 따라서 적어도 하나의 매장은 다른 매장들과 평균 매출이 다르다.  
즉, 매장 간 평균 매출에는 유의미한 차이가 있다고 할 수 있다.
#### Kruskal-Wallis Test  
매장 간 평균 매출의 분포가 동일한지 확인하는 비모수적인 방법 중 하나인 Kruskal-Wallis 테스트를 수행
from scipy.stats import kruskal

# Kruskal-Wallis Test
h_statistic, p_value_kruskal = kruskal(*[train['Sales'][train['Store'] == store] for store in train['Store'].unique()])

print(f'Kruskal-Wallis h_statistic: {h_statistic}')
print(f'Kruskal-Wallis p-value: {p_value_kruskal}')
Kruskal-Wallis Test 결과를 보면 h-statistic 값이 매우 크고 p-value가 0.0으로 나왔다.  
이는 Kruskal-Wallis Test를 통해 각 매장의 평균 매출 분포가 모두 동일하다는 귀무가설을 기각할 충분한 증거가 있다는 것을 나타낸다.

해석:
Kruskal-Wallis h-statistic 값이 클수록 그룹 간의 차이가 크다는 의미다. 여기서는 매장 간 차이가 크다는 것을 나타낸다.  
p-value가 0.05보다 작으므로 귀무가설을 기각한다. 따라서 적어도 하나의 매장은 다른 매장들과 평균 매출 분포가 다르다.  
즉, Kruskal-Wallis Test 결과에 따르면 매장 간 평균 매출 분포에는 유의미한 차이가 있다고 할 수 있다.
#### Tukey's HSD (Honestly Significant Difference)  
ANOVA에서 평균이 다르다고 나온 경우, 어떤 매장이 다른 매장들과 다른지 확인하기 위해 Tukey's HSD를 수행

from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Tukey's HSD
tukey_result = pairwise_tukeyhsd(endog=train['Sales'], groups=train['Store'], alpha=0.05)
print('Tukey\'s HSD:')
print(tukey_result)
meandiff: 각 그룹 간의 평균 차이  
p-adj: 수정된 p-value입니다. 이 값이 0.05보다 작으면 귀무가설을 기각한다.  
이 결과에 따르면, 매장 간에 평균 매출에 차이가 있다고 볼 수 있는 그룹이 있고, 차이가 없다고 볼 수 있는 그룹도 있다.  
reject 열이 True인 경우에만 귀무가설을 기각하므로 유의미한 차이가 있는 그룹이다.  

예시
매장 3, 4, 7, 9, 11, 12, 15, 16, 17, 18, 19, 20 등은 다른 매장과 유의미한 평균 매출 차이가 있다.  
반면에 매장 1, 2, 5, 6, 8, 13, 14, 21 등은 다른 매장과 유의미한 평균 매출 차이가 없다.
### 검정 결론
귀무가설을 기각함으로써 통계적으로 유의미한 차이가 있다고 판단할 수 있다.  
ANOVA 및 Tukey's HSD 검정을 통해 여러 매장 간의 평균 매출에 차이가 있다고 결론내렸다.  

즉, 귀무가설(H0)은 "모든 매장 간의 평균 매출은 동일하다"라는 가설이었고,  
이를 기각함으로써 "적어도 하나의 매장에서 다른 매장들과 평균 매출이 다르다"는 대립가설(H1)을 채택할 수 있게 된다.  
따라서, 각 매장 간에는 통계적으로 유의미한 평균 매출 차이가 있다고 해석할 수 있다.  

이는 특정 매장이 다른 매장들과 매출에서 다르다는 것을 나타낸다.
## 회귀분석
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
train.info()
### 선형 회귀 모델
#### 데이터 선택
features = ['Customers', 'Open', 'Promo']
X = train[features]
y = train['Sales']
#### 학습 데이터와 테스트 데이터로 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=42)
print(f'X train shape : {X_train.shape}')
print(f'y train shape : {y_train.shape}')
print(f'X test shape : {X_test.shape}')
print(f'y test shape : {y_test.shape}')
#### 선형 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
#### 모델 평가
y_pred = model.predict(X_test)
y_pred
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print(f'Mean Absolute Error: {mae}')
- Mean Squared Error (MSE)  
모델이 예측한 값과 실제 값 간의 평균 제곱 오차     
MSE: 2218983.01  
더 작은 MSE 값은 모델의 예측이 실제 값에 더 가깝다는 것을 나타낸다.
- R-squared (결정 계수)  
종속 변수의 분산에서 설명된 분산의 비율
R-squared: 약 0.85  
모델이 종속 변수의 변동의 약 85%를 설명한다는 것을 의미한다.
- Mean Absolute Error (MAE)  
예측값과 실제값 간의 평균적인 절대적인 오차  
MAE: 약 986.47  
이 값이 만족스럽다면 모델이 예측한 값과 실제 값 간의 평균적인 오차가 작다고 볼 수 있다.   
하지만 MAE의 해석은 주어진 문제와 데이터의 특성에 따라 달라질 수 있다.
#### 회귀 계수 확인
coefficients = pd.DataFrame({'Variable': features, 'Coefficient': model.coef_})
print(coefficients)
- 계수 (Coefficients)  
각 특성에 대한 회귀 계수  
Customers: Open이 1 증가할 때마다 약 6.08만큼 Customers가 증가한다.  
Open 및 Promo도 유사한 방식으로 해석할 수 있다.  
Open이나 Promo가 0이면 해당 효과는 없다.  

각 계수는 해당 특성이 종속 변수에 미치는 영향을 나타낸다.  
이 모델의 성능은 R-squared 값이 높고, 계수들이 통계적으로 유의미한 것처럼 보인다.  
#### 시각화
실제 매출과 예측 매출 비교
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Sales')
plt.ylabel('Predicted Sales')
plt.title('Actual vs. Predicted Sales')
plt.show()
### 모델 적합도 검증
#### 교차 검증 (Cross Validation)
데이터를 여러 부분으로 나누어 모델을 여러 번 학습하고 평가하는 과정  
K-fold 교차 검증이 많이 사용
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print("교차 검증 점수:", scores)
#### 잔차 분석 (Residual Analysis)
모델이 얼마나 잘 예측했는지를 확인하기 위해 실제 값과 예측값 간의 잔차(residual) 분석
residuals = y_test - y_pred
plt.scatter(y_pred, residuals)
plt.xlabel("predicted value")
plt.ylabel("residual")
plt.show()
#### 회귀 진단 그래프 (Regression Diagnostic Plots)
잔차의 정규성, 등분산성, 독립성 등을 확인하기 위한 다양한 그래프 그린다.
sns.residplot(x=y_pred, y=residuals)
#### QQ 플롯 (Quantile-Quantile Plot)
잔차의 정규성을 확인하기 위한 그래프
잔차가 정규분포를 따르는지 확인
import statsmodels.api as sm
sm.qqplot(residuals, line='s')
데이터 세트는 정규 분포를 따르지 않는다.
- QQ 플롯은 직선에서 벗어나 있다.

데이터 세트에는 특정 특징이 있다.
- QQ 플롯의 왼쪽 꼬리가 굵다. 
- 이는 데이터 세트에 작은 값이 더 많다는 것을 의미한다.

두 데이터 세트는 유사한 분포를 따릅니다.
- 두 데이터 세트의 QQ 플롯은 서로 가까이 있다.
#### 레버리지-잔차 제곱 그래프 (Leverage-Residual Squared Plot)
레버리지와 표준화 잔차의 제곱을 표시하여 이상치를 찾는 데 사용
from statsmodels.stats.outliers_influence import OLSInfluence

X = train[features]
y = train['Sales']
X = sm.add_constant(X)
ols = sm.OLS(y, X).fit()
influence = OLSInfluence(ols)
leverage = influence.hat_matrix_diag

plt.scatter(leverage, ols.resid**2)
plt.xlabel('Leverage')
plt.ylabel('Residuals Squared')
plt.title('Leverage vs Residuals Squared')
plt.show()
데이터 세트에는 이상치가 있을 가능성이 있다.
- 레버리지가 높고 표준화 잔차가 큰 데이터 포인트가 있다.

데이터 세트에는 특정 특징이 있다.
- 레버리지가 낮고 표준화 잔차가 큰 데이터 포인트가 있다.
- 이는 데이터 세트에 영향력이 큰 잔차가 있음을 의미한다.
#### Cook's Distance
회귀 모델에서의 이상치를 찾는 데 사용되는 통계량  
Cook's Distance가 큰 데이터 포인트는 모델에 큰 영향을 미친다.
influence = OLSInfluence(ols)
cooks_distance = influence.cooks_distance[0]

plt.stem(cooks_distance)
데이터 세트에는 이상치가 있을 가능성이 있다.
- Cook's Distance 값이 큰 데이터 포인트가 있다.
### 다중공선성 진단
VIF 등을 통한 독립 변수 간 상호 의존성 검사
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = train[features]
X = sm.add_constant(X) # 상수항 추가
# VIF 계산
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

vif_data
VIF 결과를 살펴보면 각 변수의 VIF가 모두 10 미만으로 나타났다.  
일반적으로는 VIF가 10 이상이면 다중공선성이 우려되지만, 여기서는 안전하게 다중공선성 문제가 없다고 판단할 수 있다.  
따라서 회귀모델의 독립 변수 간 상관 관계는 다중공선성으로 인해 크게 왜곡되지 않았고, 모델이 적절하게 적합되었다고 할 수 있다.
### 예측
#### 피처와 타겟 변수 선택
features = ['Open', 'Promo']
target = 'Sales'
#### train 데이터에서 선택한 피처와 타겟 변수 추출
X_train = train[features]
y_train = train[target]
#### test 데이터에서 선택한 피처 추출
X_test = test[features]
#### 모델 초기화 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
#### test 데이터에 대한 예측
y_pred = model.predict(X_test)
y_pred
#### 예측된 Sales를 확인
predictions = pd.DataFrame({'Predicted_Sales': y_pred})
predictions
## 회고
데이터를 엄청 뜯어보고 어떻게 처리할까 고민을 하니까 생각보다 재밌었다.  
store 데이터에 대해서 조금 더 처리를 해보고 싶었는데 감이 안잡히고 했고, 합친다고 해서 test 데이터에 대해서 예측을 잘할까?  
store 데이터가 결측치가 생각보다 많은데 어떻게 처리하지? 이걸 처리하는 것도 엄청난 시간이 들 거 같아서 시간이 한정적이여서 다음에 해보기 정했다.  

그리고 여러가지 그래프들을 써봤는데 아직 자세히 몰라서 조금 더 공부해야할 거 같다.  
여러가지 방법들을 프로젝트 하면서 알게되니까 좋으면서도 공부하 게 점점 늘어나니까 어쩔땐 두렵기도 하다.  
공부를 끊임없이 해야하는 건 알지만 상당히 많아서..쉬는 날이 쉬는 날 같지 않달까?  

평소에는 그냥 전처리, 스케일링, 모델링 끝! 이런 느낌이였는데 조금 더 체계적으로 (?) 데이터를 다룰 수 있게 된 거 같다.  
미숙하지만? 알고 있다는 게 중요하다고 생각하기 때문에 !! 많은 걸 알게 돼서 좋다.  

완성도는 그렇게 좋다? 할 수 없지만 열심히 했다..  
